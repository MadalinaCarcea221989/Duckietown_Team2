{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Open AI Gym\n",
    "\n",
    "## Environment Basics\n",
    "\n",
    "Open AI Gym allows you to create virtual environments to simulate and agent interacting with the environment in order to create algorithms to control the way the angent behaves.\n",
    "I this example we will look at the cart pole environment. The goal is to try and balance the pole on the cart (you will see what this looks like at the end of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the gym \n",
    "import gym\n",
    "\n",
    "# To create an instance of an environment, use gym.make() with the environment's name\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Observation Space\n",
    "\n",
    "Our agent (in this case the cart, later on our duckiebots) interacts with the environment in a loop of observation, action, and reward. The observation is a piece of information or data about the current state of the environment. It is the information that the agent has about the environment at each step. The \"observation space\" defines the format and range of possible observations that an agent can expect to receive from the environment.\n",
    "\n",
    "When you query the `observation_space` of an environment, it tells you the structure of the observations that will be provided to the agent. This is important because the agent's decision-making process depends on understanding what it observes in the environment.\n",
    "\n",
    "Let's take a closer look at what the observation space represents in our 'CartPole-v1' example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see what the observation space looks like:\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will output the structure and bounds of the observation space for the CartPole environment. In Gym environments, observation spaces can be of different types, such as `Discrete` (finite possibilities), `Box` (a n-dimensional box), `Tuple` (a combination of simpler spaces), and others.\n",
    "\n",
    "For 'CartPole-v1', the observation space is a `Box` type. This means it is an n-dimensional box, so the observations will be an array of numbers representing the quantities of the environment's state. Specifically, for CartPole, it will return an array of 4 numbers, which typically represent:\n",
    "\n",
    "1. The cart position (a float, which can range from -4.8 to 4.8).\n",
    "2. The cart velocity (a float, which has no specific limits in the CartPole environment).\n",
    "3. The angle of the pole with respect to the cart (a float, ranging approximately from -0.418 rad (-24 deg) to 0.418 rad (24 deg)).\n",
    "4. The rotation rate of the pole (a float, with no specific limits in the CartPole environment).\n",
    "\n",
    "The bounds of this box are given as high and low values for each component in the state vector. For the CartPole, these bounds are part of the environment's definition and are set to reasonable values that are known to contain all possible states.\n",
    "\n",
    "Understanding the observation space is crucial for designing the agent's neural network (or any other model) since the input size often matches the dimensionality of the observation space. It also influences preprocessing: knowing the range and type of each observation component can guide how you might normalize or encode the data before feeding it into your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Action Space\n",
    "\n",
    "The \"action space\" defines the set of possible actions that the agent can take in the environment. Each action is a decision made by the agent at each step of interaction, aiming to achieve a goal or maximize cumulative reward over time.\n",
    "\n",
    "Understanding the action space is crucial because it dictates how the agent can influence the environment. Different environments can have different kinds of action spaces:\n",
    "\n",
    "- **Discrete action spaces** where the actions are a finite set of distinct values or commands.\n",
    "- **Continuous action spaces** (also called Box spaces in Gym) where actions are real-valued and can represent things like velocities, torques, or other continuous inputs.\n",
    "- **MultiDiscrete** or **MultiBinary** spaces for environments where the agent has to take multiple independent actions simultaneously.\n",
    "\n",
    "For our 'CartPole-v1' environment, the action space consists of two discrete actions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see what the action space looks like:\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this command will output something like `Discrete(2)`. This means there are two possible actions that the agent can take at any given step:\n",
    "\n",
    "1. `0`: Push the cart to the left\n",
    "2. `1`: Push the cart to the right\n",
    "\n",
    "These actions are the only two possibilities the agent can choose from to interact with the environment. When implementing a learning algorithm, the agent will have to decide which action to take based on the observations it receives. The learning part of reinforcement learning refers to improving this decision-making process over time, usually by trial and error.\n",
    "\n",
    "In the case of a Discrete action space like in 'CartPole-v1', it is common for agents to have a policy that maps observations to actions, choosing an action either deterministically or probabilistically from this set.\n",
    "\n",
    "For more complex or realistic environments, the action space can be more complicated. However, no matter how complex, the concept remains the same: the action space defines what the agent can do at any given step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the Environment\n",
    "\n",
    "Before you start interacting with an environment, or at the beginning of a new episode, it is necessary to reset the environment to an initial state. The environment's `reset()` method is responsible for this. Resetting the environment is important for two reasons:\n",
    "\n",
    "1. **Starting Point:** It provides a clean starting point for the agent to begin a new episode.\n",
    "2. **Consistency:** It ensures consistency in the starting conditions for each episode, which is important for comparing the performance of different strategies over time.\n",
    "\n",
    "Here's how you can reset the 'CartPole-v1' environment and inspect the initial observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The environnemt must be reset before it can be used\n",
    "observation = env.reset()\n",
    "# To see what the observation space looks like:\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call `env.reset()`, it returns the initial observation, which represents the initial state of the environment. In the CartPole environment, this is a 4-dimensional vector corresponding to the position and velocity of the cart, and the angle and rotation rate of the pole.\n",
    "\n",
    "\n",
    "Running the above commands will reset the environment and print the initial state of the environment. This state is a NumPy array with four numbers. For example, you might see something like `[0.03073904, 0.00145035, 0.04207708, -0.03131252]`. These numbers are sampled from a uniform distribution over a small range, ensuring some variety in the starting conditions of each episode. Here's what each number represents:\n",
    "\n",
    "1. **Cart Position:** Close to the center of the track.\n",
    "2. **Cart Velocity:** Near zero, indicating the cart is almost stationary.\n",
    "3. **Pole Angle:** Slightly tilted from vertical.\n",
    "4. **Pole Velocity At Tip:** Near zero, suggesting the pole is not rotating significantly.\n",
    "\n",
    "This initial observation is what the agent uses to decide its first action. After the environment is reset and the initial observation is received, the agent is ready to start the episode and begin the cycle of taking actions and receiving subsequent observations and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Actions and Stepping Through the Environment\n",
    "\n",
    "Once the environment is reset and the agent has an initial observation, the next step is to start interacting with the environment. This interaction happens through a sequence of steps, where the agent takes an action and the environment responds with a new state, a reward, and some additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To take an action, use the step method which returns four values:\n",
    "# observation (object), reward (float), done (boolean), info (dict)\n",
    "action = env.action_space.sample() # take a random action\n",
    "print(action) \n",
    "observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these returned values has a specific significance:\n",
    "\n",
    "1. **observation (object)**: This is the new state of the environment after the action has been taken. It provides the agent with the updated information necessary to decide the next action.\n",
    "\n",
    "2. **reward (float)**: This is the reward resulting from taking the action. The agent's goal is to maximize the total reward it receives over time. In 'CartPole-v1', the agent receives a reward of 1 for every timestep that the pole remains upright.\n",
    "\n",
    "3. **done (boolean)**: This flag indicates whether the episode has ended. An episode ends when the agent reaches a terminal state, which in the case of 'CartPole-v1' happens when the pole falls over or the cart moves too far from the center.\n",
    "\n",
    "4. **info (dict)**: This dictionary can provide auxiliary diagnostic information. In many Gym environments, this is an empty dictionary, but the infrastructure is there to provide more detail in more complex scenarios. The Duckietown environment does provide useful extra information here.\n",
    "\n",
    "Let's break down the action step more concretely:\n",
    "\n",
    "- `env.action_space.sample()` is a function provided by Gym that samples an action randomly from the action space of the environment. This is useful, especially when you're testing the environment and want to see how it responds to different actions. We just used random actions for demonstration purposes, you will be creating an algorithm to choose appropriate actions based on the observed state.\n",
    "\n",
    "- `env.step(action)` is the function that actually executes the action within the environment. You pass in the action that the agent has decided to take (in this case, a random action), and it returns the new observation, the reward received, and whether or not the episode has finished.\n",
    "\n",
    "By repeatedly calling `env.step(action)` with the agent's chosen actions, you simulate the environment's dynamics and allow the agent to experience a sequence of states, rewards, and decisions, which is how it learns over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main Loop\n",
    "\n",
    "The main loop of the simulation allows the agent to continuously interact with the environment until a terminal state is reached. Here, we will go through the process of setting up and running the main loop with our `CartPole-v1` environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gym module\n",
    "import gym  \n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()\n",
    "\n",
    "done = False\n",
    "# Run the simulation until the episode is done\n",
    "while not done:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # print the action and observation in a nice f string\n",
    "    print(f\"Action taken: {action}\")\n",
    "    print(f\"Observation: {observation}\")\n",
    "\n",
    "    # Get the new state and reward from the environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this code, it performs the following steps in the while loop:\n",
    "\n",
    "1. **Random Action Selection:** At each step, the agent (which is not yet implementing a learning algorithm) selects an action at random from the available action space. This action is then printed for us to see.\n",
    "\n",
    "2. **Environment Step:** The selected action is applied to the environment using the `env.step(action)` method. This method executes the action and returns the new observation (state of the environment), the reward received for the action, and whether the episode has concluded (`done`).\n",
    "\n",
    "3. **Episode Termination:** The loop continues until the `done` flag is returned as True, which happens when the pole falls too much, or the cart moves off the center.\n",
    "\n",
    "4. **Environment Closure:** Finally, after the loop concludes (the episode ends), we clean up by calling `env.close()`. This is good practice as it properly releases the resources the environment may be using.\n",
    "\n",
    "The primary purpose of this code is to illustrate how the agent-environment interaction loop works. In a more complex implementation, the agent would use observations to decide the next action or learn over time which actions lead to higher rewards, instead of randomly sampling actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering the Environment\n",
    "\n",
    "In order to see what we are doing we can render the environment in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gym module\n",
    "import gym  \n",
    "# required to display the environment in the notebook\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function to display an image in Jupyter notebook\n",
    "def show_array(arr):\n",
    "    img = Image.fromarray(arr, 'RGB')\n",
    "    display(img)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()\n",
    "\n",
    "done = False\n",
    "# Run the simulation until the episode is done\n",
    "while not done:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Get the new state and reward from the environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    # Render the current frame as an rgb array\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    # Clear the previous output and display the new frame\n",
    "    clear_output(wait=True)\n",
    "    show_array(frame)\n",
    "\n",
    "# close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running for a Fixed number of Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gym module\n",
    "import gym  \n",
    "# required to display the environment in the notebook\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function to display an image in Jupyter notebook\n",
    "def show_array(arr):\n",
    "    img = Image.fromarray(arr, 'RGB')\n",
    "    display(img)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()\n",
    "\n",
    "done = False\n",
    "# Run the simulation for x steps\n",
    "num_steps = 1000\n",
    "for i in range(num_steps):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Get the new state and reward from the environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    # Render the current frame as an rgb array\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    # Clear the previous output and display the new frame\n",
    "    clear_output(wait=True)\n",
    "    show_array(frame)\n",
    "\n",
    "    # If the episode is done, reset the environment\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running for Multiple Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gym module\n",
    "import gym  \n",
    "# required to display the environment in the notebook\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function to display an image in Jupyter notebook\n",
    "def show_array(arr):\n",
    "    img = Image.fromarray(arr, 'RGB')\n",
    "    display(img)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Run the simulation for x episodes\n",
    "num_episodes = 10\n",
    "for i in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    # Run the simulation until the episode is done\n",
    "    while not done:\n",
    "        # Take a random action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Get the new state and reward from the environment\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Render the current frame as an rgb array\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        # Clear the previous output and display the new frame\n",
    "        clear_output(wait=True)\n",
    "        show_array(frame)\n",
    "\n",
    "        # If the episode is done, reset the environment\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try this with another Gym Environment\n",
    "\n",
    "### Here are some you can try:\n",
    "\n",
    "- MountainCar-v0\n",
    "- Pendulum-v1\n",
    "- MountainCarContinuous-v0\n",
    "\n",
    "Make sure you look at the differences in the observation and action space for these environments.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
